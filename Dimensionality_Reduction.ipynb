{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dimensionality Reduction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNTP58/ar4NdDw4DIg+2A2J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aswinibishoyi/3.Data-Preprocessing/blob/master/Dimensionality_Reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ6HIrGXwvEX",
        "colab_type": "text"
      },
      "source": [
        "The number of input variables or features for a dataset is referred to as its dimensionality. Dimensionality reduction reduces the number of input variables. More input features often make a predictive modeling task more challenging So, referred to as the curse of dimensionality.\n",
        "A classification or regression dataset uses it in order to better fit a predictive model.\n",
        "So, i)Large numbers of input features can cause poor performance for machine learning algorithms, ii)Dimensionality reduction reduces the number of input features, iii)Dimensionality reduction methods include feature selection, linear algebra methods, projection methods, and autoencoders.\n",
        "\n",
        "Techniques for Dimensionality Reduction\n",
        "1)Feature Selection Methods\n",
        "2)Linear Algebra Methods\n",
        "3)Projection Methods\n",
        "4)Autoencoder Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5zbfrl0CTc2",
        "colab_type": "text"
      },
      "source": [
        "# Calculate Principal Component Analysis (PCA) from Scratch in Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "varSoJ90CZfQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "9efcead0-992c-4cf2-f46f-9d93b03623c4"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import mean\n",
        "from numpy import cov\n",
        "from numpy.linalg import eig\n",
        "# define a matrix\n",
        "A = array([[1, 2], [3, 4], [5, 6]])\n",
        "print(A)\n",
        "# calculate the mean of each column\n",
        "M = mean(A.T, axis=1)\n",
        "print(M)\n",
        "# center columns by subtracting column means\n",
        "C = A - M\n",
        "print(C)\n",
        "# calculate covariance matrix of centered matrix\n",
        "V = cov(C.T)\n",
        "print(V)\n",
        "# eigendecomposition of covariance matrix\n",
        "values, vectors = eig(V)\n",
        "print(vectors)\n",
        "print(values)\n",
        "# project data\n",
        "P = vectors.T.dot(C.T)\n",
        "print(P.T)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "[3. 4.]\n",
            "[[-2. -2.]\n",
            " [ 0.  0.]\n",
            " [ 2.  2.]]\n",
            "[[4. 4.]\n",
            " [4. 4.]]\n",
            "[[ 0.70710678 -0.70710678]\n",
            " [ 0.70710678  0.70710678]]\n",
            "[8. 0.]\n",
            "[[-2.82842712  0.        ]\n",
            " [ 0.          0.        ]\n",
            " [ 2.82842712  0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}